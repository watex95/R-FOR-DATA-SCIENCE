---
title: "Subset selection - Practice H/W"
author: "Hillary"
date: "1/29/2020"
output: html_document
---

#### 1. Load the cpus dataset from the MASS package

```{r}
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
# inspect data
sample_n(cpus,5)
```

#### 2. Use syct, mmin , mmax , cach , chmin, chmax as the predictors (independent variables) to predict performance (perf)
The function summary() reports the best set of variables for each model size. From the output, an asterisk specifies that a given variable is included in the corresponding model.
For example, it can be seen that the best 2-variables model contains only mmax and cach variables (perf ~ mmax + cach). The best three-variable model is (perf ~ mmax + cach + mmin), and so forth.
A natural question is: which of these best models should we finally choose for our predictive analytics?

```{r}
models <- regsubsets(perf~syct + mmin + mmax + cach + chmin + chmax, data = cpus, nvmax = 6)
summary(models)
```



#### 3. Perform best subset selection in order to choose the best predictors from the above predictors. What is the best model obtained according to Cp, BIC, and adjusted R2?
Model selection criteria: Adjusted R2, Cp and BIC
```{r}
res.sum <- summary(models)
res.sum$cp
res.sum$bic

data.frame(
  Adj.R2 = which.max(res.sum$adjr2),
  CP = which.min(res.sum$cp),
  BIC = which.min(res.sum$bic)
)
```
As shown above, adjusted R2,BIC and Cp criteria, tells us that the best model is the one with all the 5 predictor variables.

#### 4.Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained for each criteria
```{r}
plot(res.sum$bic, xlab="Parameter", ylab="BIC")
plot(res.sum$cp, xlab="Parameter", ylab="CP")
```

#### 5. Repeat using forward stepwise selection and also using backwards stepwise selection.How does your answer compare to the best subset results?

Fit the full model
```{r}
library(MASS)
full.model <- lm(perf ~syct + mmin + mmax + cach + chmin + chmax, data = cpus)
full.model
```

Stepwise regression model
```{r}
step.model <- stepAIC(full.model, direction = "both",trace = FALSE)
summary(step.model)  
models <- regsubsets(perf ~syct + mmin + mmax + cach + chmin + chmax, data = cpus,
                     nvmax = 6,
                     method = "seqrep")
summary(models)

```

Backward elimination
```{r}
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 6)
# Train the model
step.model <- train(perf ~syct + mmin + mmax + cach + chmin + chmax, data = cpus,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:5),
                    trControl = train.control)
step.model$results
```



```{r}

```

